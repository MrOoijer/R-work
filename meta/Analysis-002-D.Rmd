---
title: "Analyse versie II"
author: "Jan van Rongen"
date: "2015-03-05"
output: pdf_document
file: "Analysis-002-D.Rmd"
---
0. Introductie en samenvatting
---------------------------

Dit is mijn analyse gebaseerd op de spreadsheet. De gegevens uit de eerste tabel van deze spreadsheet zijn gekopieerd naar de csv-file "cases.csv". De kolomnamen in deze file zijn door mij toegevoegd. 

Deze analyse volgt de lijnen van de tutorial voor het metafor pakket bij R (*Conducting Meta-Analyses in R with the metafor Package, Wolfgang Viechtbauer, Maastricht University. Journal of Statistical Software; August 2010, Volume 36, Issue 3.*). 

De meta-analyse is uitgevoerd op de 39 studies die in de genoemde tabel voldoende gegevens bevatten om een model te kunnen maken. Dit is dus inclusief studies waarvoor de spreadsheet 0 positieven rapporteert.  Met behulp van het Random Effect Sizes model vinden we een significant effect (0.71, CI=[0.59, 0.86]) voor het Relatieve Risico. 

De funnel-plot, de trim-fill test en een plot van p-waarden geven geen duidelijke indicatie voor publication bias. Integendeel: de gegevens blijken te divers om daar op statistische gronden iets over te kunnen zeggen. 

De heterogeniteit van de effecten in de studies is zodanig groot, dat een verdere analyse met een Mixed Effects Model gewenst lijkt. De oorzaak kan er in loiggen dat de ene studie naar meer soorten effecten kijkt dan de andere. 

1. Analyse van de aard van de input
-------------------

Data-voorbeeld:

```{r, message=FALSE,warning=FALSE, echo=FALSE}
library(metafor)
mydata<-read.csv("cases.csv", sep=";", header=TRUE)
# delete incomplete cases
mydata<-mydata[complete.cases(mydata),]
print(mydata[1:3,], row.names=FALSE)
```

De tpos- en cpos-kolommen zijn de totale aantallen bijwerkingen in de (resp.) test- en controlegroep. Omdat niet alle studies dezelfde groep bijwerkingen hebben gerapporteerd, kunnen we niet uitgaan van een gemeenschappelijk model voor de trials. Met andere woorden: de te verwachten Odds ratio's of Risk Ratio's zullen afhankelijk zijn van factoren die per trial al kunnen verschillen. We kunnen dus gebruik maken van een random effect model, maar niet van een fixed effect model om een meta-analyse te maken. Omdat het rapporteren van géén bijwerkingen er op wijst dat (wellicht) in die studies er weinig verschillende bijwerkingen zijn bekeken, lijkt het me verstandig in dit document ook een aparte analyse zonder de "dubbel-0"-observaties te maken.

Als de ene studie bijeffecten/complicaties van de soort a, b en c rapporteert, en een andere studie alleen a, dan zijn die studies niet goed onderling vergelijkbaar, tenzij, maar dat lijkt wel een heel drastische aanname, de Risk ratio's in de populatie voor alle soorten complicaties identiek aan elkaar zijn. 

2. Random Effect Sizes Modellen
----------------------------

###Met "dubbel-0" observaties

```{r, echo=FALSE}
# data maken - RR
dat.RR0 <- escalc(measure = "RR", ai = tpos, bi = ttotal-tpos, ci = cpos, di = ctotal-cpos
              , data = mydata, append = TRUE)
res.RR0 <- rma(yi, vi, data = dat.RR0) # make the model
ci.RR0 <- confint(res.RR0) # confidence intervals of important measures
# 
mydata.rs <- mydata[mydata$tpos+mydata$cpos>0,]
dat.RR <- escalc(measure = "RR", ai = tpos, bi = ttotal-tpos, ci = cpos, di = ctotal-cpos
              , data = mydata.rs, append = TRUE)
res.RR <- rma(yi, vi, data = dat.RR) # make the model
ci.RR <- confint(res.RR) # confidence intervals of important measures
# data maken - OR
dat2 <- escalc(measure = "OR", ai = tpos, bi = ttotal-tpos, ci = cpos, di = ctotal-cpos
              , data = mydata, append = TRUE)
res.OR0 <- rma(yi, vi, data = dat2)
ci.OR0<-confint(res.OR0)
dat3 <- escalc(measure = "OR", ai = tpos, bi = ttotal-tpos, ci = cpos, di = ctotal-cpos
              , data = mydata.rs, append = TRUE)
res.OR<- rma(yi, vi, data = dat3)
ci.OR<-confint(res.OR)
# print 
print(dat.RR0[1:3,], row.names=FALSE); print(res.RR0); print(ci.RR0)
```

###Zonder "dubbel-0" observaties

```{r, echo=FALSE}
print(dat.RR[1:3,], row.names=FALSE); print(res.RR); print(ci.RR)
```

####Het resultaat

Hoewel de Risk Ratio (zie de grafieken hieronder) niet verandert, neemt de heterogeniteit wel toe! Enig nadenken: dat komt omdat er 12 homogene observaties uitgehaald worden. 

Dezelfde berekeningen doen we voor de (log) Odd Ratio - hier niet afgedrukt. Die gebruiken we later bij de funnel plots. 
 
De heterogeniteit staat in de tau^2 statistic. De I^2 statistic (Higgins & Thompson 2002) schat het percentage van de totale variabiliteit dat kan worden toegerekend aan de variabiliteit van de onderliggende testen. Deze maat, tezamen met de Q-test voor heterogeniteit suggereert duidelijk een grote heterogeniteit onder de werkelijke effecten, die een verder onderzoek *in een mixed effects model* zeer wenselijk maakt.

3. Forest plots
-------------

We maken er 2, zo veel mogelijk standaard. RR met en zonder de "dubbel-0's":

###Alle observaties

```{r,fig.height=9.5,fig.width=8, echo=FALSE}
# forest plot
N=dim(mydata)[1]+2
forest(res.RR0, slab = paste(dat.RR0$author, dat.RR0$year, sep = ", ")
       , xlim = c(-16, 6), at = log(c(0.05, 0.25, 1, 4)), atransf = exp
       , ilab = cbind(dat.RR0$tpos, dat.RR0$ttotal, dat.RR0$cpos, dat.RR0$ctotal)
       , ilab.xpos = c(-9.5, -8, -6, -4.5), cex = 0.75)
# legends on this plot
op <- par(cex = 0.75, font = 2)
text(c(-9.5, -8, -6, -4.5), N, c("+", "Total", "+", "Total"))
text(c(-8.75, -5.25), N+1, c("Treated", "Control"))
text(-16, N, "Author(s) and Year", pos = 4)
text(6, N, "Relative Risk [95% CI]", pos = 2)
```

###Niet-nul observaties

```{r,fig.height=8,fig.width=8, echo=FALSE}
# forest plot
N=dim(mydata.rs)[1]+2
forest(res.RR, slab = paste(dat.RR$author, dat.RR$year, sep = ", ")
       , xlim = c(-16, 6), at = log(c(0.05, 0.25, 1, 4)), atransf = exp
       , ilab = cbind(dat.RR$tpos, dat.RR$ttotal, dat.RR$cpos, dat.RR$ctotal)
       , ilab.xpos = c(-9.5, -8, -6, -4.5), cex = 0.75)
# legends on this plot
op <- par(cex = 0.75, font = 2)
text(c(-9.5, -8, -6, -4.5), N, c("+", "Total", "+", "Total"))
text(c(-8.75, -5.25), N+1, c("Treated", "Control"))
text(-16, N, "Author(s) and Year", pos = 4)
text(6, N, "Relative Risk [95% CI]", pos = 2)
```


4. Funnel plots
-------------
Met inschatting publication bias volgens het trim-fill algoritme.


```{r,fig.height=9,fig.width=8, echo=FALSE}

op=par(mfrow=c(2,2), cex = 0.75, font = 2)

funnel(trimfill(res.RR0), main = "RE / RR Model, double-0 included"
       , xlim=c(-4,4), ylim=c(0,2))
funnel(trimfill(res.RR), main = "RE / RR Model, double-0 excluded"
       , xlim=c(-4,4), ylim=c(0,1.5))
funnel(trimfill(res.OR0), main = "RE / OR Model, double-0 included"
       , xlim=c(-4,4), ylim=c(0,2))
funnel(trimfill(res.OR), main = "RE / OR Model, double-0 excluded"
       , xlim=c(-4,4), ylim=c(0,1.5))

par(op)

```

####Conclusies

Het is nu duidelijk, met de inzet van trimfill bij de odds ratio, dat de eerst ingeschatte ontbrekende publicaties een gevolg zijn van de "dubbel-0"-observaties. Dat zijn dus toch outliers, die het resultaat eerder vertroebelen dan verklaren. Wat er overblijft aan mogelijke publicatie-bias is nauwelijks de moeite waard om -op deze wijze- verder uit te diepen. 

Waarom RR geen, en OR wel ontbrekende resultaten laat zien is mij niet duidelijk - zo veel verschillen die maten toch niet? 


###p-values versus effect sizes

Een achteraf-gedachte: laten we eens naar de p-waarden van de input kijken. Nemen we de verschillende onderzoeken met weglating van de "dubbel-0" resultaten, dan kunnen we de Chi-kwadraat uitrekenen voor de (2 x 2) test of er een significant verschil is. Deze kunnen we dan weer uitzetten tegen een maat van het effect, bijvoorbeeld de log van het relatieve risiso. Dat ziet er als volgt uit.

```{r,fig.height=5,fig.width=8, echo=FALSE}
#
# plots
#
tst<-dat.RR

fun<-function(i){ 
  input<-tst[i,]
  a<-as.numeric(c(input[3], input[5], input[4]-input[3], input[6]-input[5]))
  a<-matrix(a,2,2)
  xqs<-chisq.test(a, correct=FALSE, simul=T, B=8000)

  return(xqs$p.value)
}

res<-sapply(1:dim(tst)[1], fun)
op=par(mfrow=c(1,2), cex = 0.75, font = 2)
plot(res, (tst$yi), xlim=c(0, 0.1), main="Bootstrapped Chi-sq. 2x2 test\n zoomed in on p < 0.1", 
     xlab="p-value", ylab="log RR", col=ifelse(res<0.05, "red", "blue"), pch=19, cex=1.25)
abline(v=0.05, col="grey")
abline(h=0, col="grey")
abline(h=log(0.71), lty=2, col="grey")

plot(res, (tst$yi), col=ifelse(res<0.05, "red", "blue"),
     cex=1.25, pch=19, xlab="p-value", ylab="log RR", main="Idem, all values")
abline(v=0.05, col="grey")
abline(h=0, col="grey")
abline(h=log(0.71), lty=2, col="grey")

par(op)
```

####Opmerkingen

Terwijl we bij de meeste vergelijkende onderzoeken eerder te maken hebben met p-waarden om-en-nabij de grens van 0.05, heeft de input hier het hele scala aan p-waarden, bevat dus zeer betekenisvolle verschillen tot volstrekt gelijkwaardige uitslagen die rondom de RR= 1 liggen. De laatste vijf zijn:

```{r, echo=FALSE}

tail(cbind(tst, p_waarde=round(res,2))[which(res >0.75),], 5)

```

Met dit soort waarden kan je niet van een p-value bias spreken. Dat wil niet zeggen dat er geen publication bias kan zijn, maar die is niet zichtbaar in de "ruis" van de vele hogere p-waarden. De bias zit er denk ik eerder in dat ze soms wel, soms niet de moeite van het publiceren waard werden gevonden. 


Tenslotte... Vragen... Opmerkingen
--------

* welke verschillende bijwerkingen worden onderkend, en hoe groot zijn die
* de I^2 test wijst 80% van de variantie in een niet te groot effect toe aan "andere oorzaken" - waarschijnlijk de soorten bijwerking als belangrijkste factor, kan die worden mee-gemodelleerd?
* dit keer heb ik geen R-code in het document afgedrukt. Die kan ik wel toegankelijk maken. Het makkelijkste is voor mij mijn github (https://github.com/MrOoijer), die houdt het dan ook actueel (functioneert tevens als archief). Het is wel openbaar, maar er komt toch nooit iemand. 


Jan
